import asyncio
import time
import uuid
import zoneinfo
from datetime import datetime
from typing import Any

from deepagents import create_deep_agent
from deepagents.backends import FilesystemBackend
from langchain.agents import AgentState, create_agent
from langchain.agents.middleware import PIIMiddleware
from langchain.messages import AIMessage
from langchain_anthropic import ChatAnthropic
from langchain_core.runnables import RunnableConfig
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import InMemorySaver
from nonebot import logger

from tools import agent_tools
from utils.configs import EnvConfig
from utils.memory import get_memory_service
from utils.subagents import fact_check_subagent


async def assistant_agent(
    system_prompt: str = "",
    user_prompt: str = "",
    use_model: str = EnvConfig.BASIC_MODEL,
    tools=None,
    response_format=None,
    middleware=None,
) -> Any:
    if not system_prompt:
        try:
            with open("prompts/system_prompt.md", encoding="utf-8") as f:
                system_prompt = f.read()
        except FileNotFoundError:
            logger.warning("âŒ æœªæ‰¾åˆ° system prompt æ–‡ä»¶: prompts/system_prompt.md")
            system_prompt = "You are a helpful assistant."
    if "gemini" in use_model.lower():
        model = ChatGoogleGenerativeAI(
            api_key=EnvConfig.OPENAI_API_KEY,
            base_url=EnvConfig.OPENAI_BASE_URL,
            model=use_model,
            streaming=False,
            max_retries=2,
            timeout=30,
        )
    elif "claude" in use_model.lower():
        model = ChatAnthropic(
            api_key=EnvConfig.OPENAI_API_KEY,
            base_url=EnvConfig.OPENAI_BASE_URL,
            model_name=use_model,
            streaming=False,
            max_retries=2,
            timeout=30,
            stop=None,
        )
    else:
        model = ChatOpenAI(
            api_key=EnvConfig.OPENAI_API_KEY,
            base_url=EnvConfig.OPENAI_BASE_URL,
            model=use_model,
            streaming=False,
            max_retries=2,
            timeout=30,
        )
    agent = create_agent(
        model=model,
        tools=tools,
        system_prompt=system_prompt,
        middleware=middleware or [],
        response_format=response_format,
        debug=EnvConfig.AGENT_DEBUG_MODE,
    )
    result = await agent.ainvoke({"messages": [{"role": "user", "content": user_prompt}]})
    if response_format:
        return result["structured_response"]
    content = ""
    for msg in result["messages"]:
        if msg.type == "ai" and not isinstance(msg.content, list):
            content += msg.content
    return content


class CustomAgentState(AgentState):
    user_id: str
    group_id: int


class FrontierCognitive:
    def __init__(self):
        self.tools = agent_tools.all_tools
        self.subagents: list = [fact_check_subagent]
        self.checkpoint = InMemorySaver()
        self.backend = FilesystemBackend(root_dir="./cache/sandbox")
        self.memory = get_memory_service()

    @staticmethod
    def load_system_prompt():
        """ä»å¤–éƒ¨æ–‡ä»¶åŠ è½½ system prompt"""
        try:
            with open("prompts/system_prompt.md", encoding="utf-8") as f:
                system_prompt = f.read()
                system_prompt = system_prompt.format(
                    name=EnvConfig.BOT_NAME,
                    current_time=datetime.now()
                    .astimezone(zoneinfo.ZoneInfo("Asia/Shanghai"))
                    .strftime("%Y-%m-%d %H:%M:%S"),
                )
                return system_prompt
        except FileNotFoundError:
            logger.error("âŒ æœªæ‰¾åˆ° system prompt æ–‡ä»¶: prompts/system_prompt.md")
            return f"You are {EnvConfig.BOT_NAME}, a helpful assistant. [é…ç½®é”™è¯¯: system promptæ–‡ä»¶ç¼ºå¤±]"
        except PermissionError as e:
            logger.error(f"âŒ æ— æƒé™è¯»å– system prompt æ–‡ä»¶: {e}")
            return f"You are {EnvConfig.BOT_NAME}, a helpful assistant. [é…ç½®é”™è¯¯: æ— è¯»å–æƒé™]"
        except UnicodeDecodeError as e:
            logger.error(f"âŒ system prompt æ–‡ä»¶ç¼–ç é”™è¯¯: {e}")
            return f"You are {EnvConfig.BOT_NAME}, a helpful assistant. [é…ç½®é”™è¯¯: æ–‡ä»¶ç¼–ç æ— æ•ˆ]"
        except KeyError as e:
            logger.error(f"âŒ system prompt æ¨¡æ¿å˜é‡ç¼ºå¤±: {e}")
            return f"You are {EnvConfig.BOT_NAME}, a helpful assistant. [é…ç½®é”™è¯¯: æ¨¡æ¿å˜é‡ç¼ºå¤±]"

    @staticmethod
    async def extract_uni_messages(response):
        """ç›´æ¥ä»å“åº”ä¸­æå– UniMessage å¯¹è±¡"""
        if not response or not isinstance(response, dict):
            logger.warning("âš ï¸ extract_uni_messages: response ä¸ºç©ºæˆ–ä¸æ˜¯å­—å…¸ç±»å‹")
            return []

        uni_messages = []
        for message in response.get("messages", []):
            if getattr(message, "type", None) == "tool" and getattr(message, "artifact", None) is not None:
                tool_name = getattr(message, "name", "unknown")
                uni_messages.append(message.artifact)
                logger.info(f"ğŸ“¤ æå– UniMessage: {tool_name} - ç±»å‹: {type(message.artifact)}")

        logger.info(f"ğŸ“¨ æ€»å…±æå–åˆ° {len(uni_messages)} ä¸ª UniMessage")
        return uni_messages

    async def inject_memory_context(self, messages, query_text: str, user_id: str, group_id: int | None):
        if not EnvConfig.MEMORY_ENABLED or not query_text.strip():
            return messages
        try:
            memory_items = await asyncio.wait_for(
                self.memory.retrieve_for_injection(
                    query=query_text,
                    user_id=user_id,
                    group_id=group_id,
                    max_items=EnvConfig.MEMORY_MAX_INJECTED_MEMORIES,
                ),
                timeout=max(0.1, EnvConfig.MEMORY_INJECT_TIMEOUT_MS / 1000),
            )
        except TimeoutError:
            logger.warning(f"âš ï¸ memory retrieval timeout for user {user_id}")
            return messages
        except Exception as e:
            logger.warning(f"âš ï¸ memory retrieval failed for user {user_id}: {type(e).__name__}: {e}")
            return messages

        if not memory_items:
            return messages
        memory_context = self.memory.format_for_injection(memory_items)
        if not memory_context:
            return messages

        prepared_messages = list(messages)
        insert_at = len(prepared_messages)
        if prepared_messages and prepared_messages[-1].get("role") == "user":
            insert_at = max(0, len(prepared_messages) - 1)
        prepared_messages.insert(insert_at, {"role": "system", "content": memory_context})
        return prepared_messages

    async def chat_agent(
        self,
        messages,
        user_id,
        user_name,
        capability: str = "minimal",
        group_id: int | None = None,
        query_text: str = "",
    ):
        if "gemini" in EnvConfig.ADVAN_MODEL.lower():
            model = ChatGoogleGenerativeAI(
                api_key=EnvConfig.OPENAI_API_KEY,
                base_url=EnvConfig.OPENAI_BASE_URL,
                model=EnvConfig.ADVAN_MODEL,
                streaming=False,
                max_retries=2,
                timeout=300,
            )
        elif "claude" in EnvConfig.ADVAN_MODEL.lower():
            model = ChatAnthropic(
                api_key=EnvConfig.OPENAI_API_KEY,
                base_url=EnvConfig.OPENAI_BASE_URL,
                model_name=EnvConfig.ADVAN_MODEL,
                streaming=False,
                max_retries=2,
                timeout=300,
                stop=None,
            )
        else:
            model = ChatOpenAI(
                api_key=EnvConfig.OPENAI_API_KEY,
                base_url=EnvConfig.OPENAI_BASE_URL,
                model=EnvConfig.ADVAN_MODEL,
                streaming=False,
                reasoning_effort=capability,
                verbosity="low",
                max_retries=2,
                timeout=300,
                use_responses_api=None,
            )
        agent = create_deep_agent(
            model=model,
            system_prompt=self.load_system_prompt(),
            tools=self.tools,
            middleware=[
                PIIMiddleware(
                    "api_key",
                    detector=r"sk-[a-zA-Z0-9]{32}",
                    strategy="block",
                )
            ],
            skills=["./sandbox/skills/"],
            interrupt_on={
                "write_file": False,
                "read_file": False,
                "edit_file": False,
            },
            backend=self.backend,
            subagents=self.subagents,
            checkpointer=self.checkpoint,
            context_schema=CustomAgentState,
            debug=EnvConfig.AGENT_DEBUG_MODE,
        )
        start_time = time.time()
        logger.info(f"Agentçƒ§çƒ¤ä¸­~ğŸ– æ€è€ƒç­‰çº§: {capability} ç”¨æˆ·: {user_name} (ID: {user_id})")
        prepared_messages = await self.inject_memory_context(
            messages, query_text=query_text, user_id=user_id, group_id=group_id
        )
        config: RunnableConfig = {
            "configurable": {
                "thread_id": uuid.uuid5(namespace=uuid.NAMESPACE_OID, name=user_id),
            }
        }
        try:
            response = await agent.ainvoke(
                {"messages": prepared_messages, "user_id": user_id, "group_id": group_id},
                config=config,
            )
        except TimeoutError as e:
            logger.error(f"âŒ Agentè¯·æ±‚è¶…æ—¶ ç”¨æˆ·{user_id}: {e}")
            return {
                "response": {"messages": [AIMessage("â±ï¸ è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•ã€‚")]},
                "total_time": time.time() - start_time,
                "uni_messages": [],
            }
        except (ConnectionError, OSError) as e:
            logger.error(f"âŒ Agentç½‘ç»œé”™è¯¯ ç”¨æˆ·{user_id}: {e}")
            return {
                "response": {"messages": [AIMessage("ğŸŒ ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•ã€‚")]},
                "total_time": time.time() - start_time,
                "uni_messages": [],
            }
        except (KeyError, AttributeError, ValueError) as e:
            logger.error(f"âŒ Agenté…ç½®/æ•°æ®é”™è¯¯ ç”¨æˆ·{user_id}: {type(e).__name__}: {e}")
            logger.exception("å®Œæ•´é”™è¯¯å †æ ˆ:")
            return {
                "response": {"messages": [AIMessage("âš™ï¸ ç³»ç»Ÿé…ç½®é”™è¯¯ï¼Œè¯·è”ç³»ç®¡ç†å‘˜ã€‚")]},
                "total_time": time.time() - start_time,
                "uni_messages": [],
            }
        except Exception as e:
            # å…¶ä»–æ„å¤–é”™è¯¯ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
            logger.error(f"âŒ Agentæ‰§è¡Œå‡ºç°æ„å¤–é”™è¯¯ ç”¨æˆ·{user_id}: {type(e).__name__}: {e}")
            logger.exception("å®Œæ•´é”™è¯¯å †æ ˆ:")
            return {
                "response": {"messages": [AIMessage("ğŸ’¥ æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚")]},
                "total_time": time.time() - start_time,
                "uni_messages": [],
            }

        uni_messages = await FrontierCognitive.extract_uni_messages(response)
        ai_messages = [msg for msg in response.get("messages", []) if getattr(msg, "type", None) == "ai"]
        final_response = ai_messages[-1] if ai_messages else AIMessage("æ™ºèƒ½ä»£ç†å¤„ç†å®Œæˆï¼Œä½†æ²¡æœ‰ç”Ÿæˆå“åº”ã€‚")

        processing_time = time.time() - start_time
        logger.info(f"Agentçƒ¤ç†Ÿäº†~ğŸ¥“ (è€—æ—¶: {processing_time:.2f}s)")

        return {
            "response": {"messages": [final_response]},
            "total_time": processing_time,
            "uni_messages": uni_messages,
        }
